import pandas as pd
import streamlit as st
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import chi2_contingency
from sklearn.feature_selection import mutual_info_regression

def main():
    colunas_necessarias = [
        'TP_FAIXA_ETARIA', 'TP_SEXO', 'TP_ESTADO_CIVIL', 'TP_COR_RACA', 'TP_NACIONALIDADE',
        'TP_ST_CONCLUSAO', 'TP_ESCOLA', 'IN_TREINEIRO', 'TP_PRESENCA_CN',
        'TP_PRESENCA_CH', 'TP_PRESENCA_LC', 'TP_PRESENCA_MT', 'NU_NOTA_CN', 'NU_NOTA_CH',
        'NU_NOTA_LC', 'NU_NOTA_MT', 'TP_STATUS_REDACAO', 'NU_NOTA_COMP1', 'NU_NOTA_COMP2',
        'NU_NOTA_COMP3', 'NU_NOTA_COMP4', 'NU_NOTA_COMP5', 'NU_NOTA_REDACAO',
        'Q001', 'Q002', 'Q003', 'Q004', 'Q005', 'Q006', 'Q007', 'Q008', 'Q009', 'Q010',
        'Q011', 'Q012', 'Q013', 'Q014', 'Q015', 'Q016', 'Q017', 'Q018', 'Q019', 'Q020',
        'Q021', 'Q022', 'Q023', 'Q024', 'Q025'
    ]

    df = pd.read_csv(
        'dados_enem/microdados_enem_2023/DADOS/MICRODADOS_ENEM_2023.csv',
        sep=';',
        encoding='latin1',
        usecols=colunas_necessarias
    )


    st.set_page_config(
        page_title="Dashboard ENEM",
        page_icon="üìä",
        layout="wide"
    )
    
    st.title("An√°lise de Dados do ENEM")
    st.subheader("Dashboard de Microdados Educacionais")
    
    tab_intro, tab_enem2023, tab_exploracao = st.tabs([
        "Introdu√ß√£o", 
        "Microdados ENEM 2023", 
        "Explora√ß√£o de Dados"
    ])
    
    with tab_intro:
        # Contexto sobre o ENEM
        st.header("Sobre o ENEM")
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("""
            **Objetivos principais:**
            - Avaliar o desempenho ao final da educa√ß√£o b√°sica
            - Facilitar acesso ao ensino superior
            - Desenvolver indicadores educacionais
            - Promover autoavalia√ß√£o dos participantes
            """)
        
        with col2:
            st.markdown("""
            **Evolu√ß√£o recente:**
            - 2009: Reformula√ß√£o metodol√≥gica
            - 2017: Mudan√ßa para provas em dois domingos
            - 2020: Primeira vers√£o digital
            - 2023: Novo formato de cadernos de prova
            """)

        
        st.divider()
        
        st.header("Objetivo do Dashboard")
        st.markdown("""
        Este dashboard tem como objetivo analisar quais parcelas da sociedade s√£o efetivamente alcan√ßadas pelo ENEM em rela√ß√£o ao acesso ao ensino superior. 
        Busca-se identificar e evidenciar poss√≠veis vieses associados a fatores socioecon√¥micos, permitindo compreender de que forma as condi√ß√µes sociais dos participantes influenciam suas oportunidades de ingresso na universidade. 
        A an√°lise visa contribuir para o debate sobre a equidade do exame e seu papel como instrumento de democratiza√ß√£o do ensino superior no Brasil.
        """)
        

        st.divider()
        
        st.header("Contextualiza√ß√£o dos Bancos de Dados")
        st.markdown("""
        Este dashboard analisa dois conjuntos de dados oficiais do INEP sobre o Exame Nacional do Ensino M√©dio (ENEM):
        """)
        
        # Container para Microdados ENEM 2023
        with st.expander("### 1. Microdados ENEM 2023", expanded=True):
            st.markdown("""
            **Fonte:** Instituto Nacional de Estudos e Pesquisas Educacionais (INEP)  
            **Per√≠odo:** Dados referentes ao exame de 2023  
            **Objetivo:** Fornecer dados individuais dos participantes do ENEM
            
            **Principais caracter√≠sticas:**
            - Dados anonimizados de participantes
            - Informa√ß√µes socioecon√¥micas detalhadas
            - Desempenho nas provas objetivas e reda√ß√£o
            - Metadados sobre itens das provas
            - Question√°rio socioecon√¥mico completo
            
            **Vari√°veis relevantes:**
            - Notas por √°rea de conhecimento (CN, CH, LC, MT)
            - Descritores da reda√ß√£o (5 compet√™ncias)
            - Informa√ß√µes demogr√°ficas (g√™nero, ra√ßa, idade)
            - Tipo de escola e situa√ß√£o de conclus√£o do EM
            - Respostas ao question√°rio socioecon√¥mico (Q001-Q025)
            
            **Limita√ß√µes:**
            - Dados pessoais removidos por LGPD
            - Exclus√£o de vari√°veis identificadoras
            - Faixas et√°rias em vez de idade exata
            """)

        st.info("""
        **Nota importante:** Os dados s√£o oficiais do INEP e seguem as diretrizes da 
        Lei Geral de Prote√ß√£o de Dados (LGPD). Informa√ß√µes identific√°veis foram 
        removidas ou anonimizadas para preservar a privacidade dos participantes.
        """)

    with tab_enem2023:
        st.header("An√°lise dos Microdados ENEM 2023")
        st.markdown("""
        Nesta se√ß√£o, ser√£o apresentados gr√°ficos e an√°lises dos microdados do ENEM 2023,
        com foco em aspectos socioecon√¥micos, desempenho e caracter√≠sticas dos participantes.
        """)

        st.subheader("Distribui√ß√£o de Situa√ß√£o por Tipo de Escola")



        # Cria uma coluna de status para cada linha (presente, ausente/eliminado)
        def classifica_status(row):
            if all(row[col] == 1 for col in ['TP_PRESENCA_CN', 'TP_PRESENCA_CH', 'TP_PRESENCA_LC', 'TP_PRESENCA_MT']):
                return 'Presente'
            elif any(row[col] == 0 or row[col] == 2 for col in
                     ['TP_PRESENCA_CN', 'TP_PRESENCA_CH', 'TP_PRESENCA_LC', 'TP_PRESENCA_MT']):
                return 'Ausente/Eliminado'
            return np.nan

        df['Status'] = df.apply(classifica_status, axis=1)

        # Mapeia os tipos de escola para nomes leg√≠veis
        mapa_escola = {1: 'P√∫blica', 2: 'Privada', 3: 'N√£o Informada'}
        df['Tipo de Escola'] = df['TP_ESCOLA'].map(mapa_escola)

        # Agrupa e conta
        agrupado = df.groupby(['Tipo de Escola', 'Status']).size().unstack(fill_value=0)
        agrupado['Total'] = agrupado.sum(axis=1)
        agrupado['% Presentes'] = agrupado.get('Presente', 0) / agrupado['Total'] * 100
        agrupado['% Ausentes/Eliminados'] = agrupado.get('Ausente/Eliminado', 0) / agrupado['Total'] * 100

        # Prepara DataFrame final
        df_resultado = agrupado.reset_index()[
            ['Tipo de Escola', '% Presentes', '% Ausentes/Eliminados', 'Presente', 'Ausente/Eliminado',
             'Total']].fillna(0)

        # Visualiza√ß√£o
        st.subheader("Percentual de Presen√ßa e Aus√™ncia/Elimina√ß√£o por Tipo de Escola")
        st.dataframe(df_resultado[["Tipo de Escola", "% Presentes", "% Ausentes/Eliminados"]])
        st.bar_chart(
            df_resultado.set_index("Tipo de Escola")[["% Presentes", "% Ausentes/Eliminados"]],
            use_container_width=True
        )

    with tab_exploracao:
        def cramers_v(x, y):
            confusion_matrix = pd.crosstab(x, y)
            chi2 = chi2_contingency(confusion_matrix)[0]
            n = confusion_matrix.sum().sum()
            phi2 = chi2 / n
            r, k = confusion_matrix.shape
            phi2corr = max(0, phi2 - ((k - 1) * (r - 1)) / (n - 1))
            rcorr = r - ((r - 1) ** 2) / (n - 1)
            kcorr = k - ((k - 1) ** 2) / (n - 1)
            return np.sqrt(phi2corr / min((kcorr - 1), (rcorr - 1)))

        with tab_exploracao:
            st.header("Escolhendo variaveis categ√≥ricas mais interessantes para clusterizac√£o")

            st.markdown("""
                Nesta se√ß√£o, buscamos identificar quais vari√°veis categ√≥ricas s√£o mais relevantes para a clusteriza√ß√£o em rela√ß√£o √†s notas do ENEM.

                O primeiro passo consiste em calcular a correla√ß√£o entre as vari√°veis categ√≥ricas utilizando o V de Cramer, a fim de identificar poss√≠veis redund√¢ncias. Caso sejam encontradas vari√°veis altamente correlacionadas, podemos optar por eliminar algumas delas para evitar sobreposi√ß√£o de informa√ß√µes.

                Em seguida, aplicamos a an√°lise de informa√ß√£o m√∫tua para determinar quais vari√°veis categ√≥ricas apresentam maior rela√ß√£o com as notas do ENEM. Dessa forma, selecionamos as vari√°veis mais significativas para compor a clusteriza√ß√£o.
            """)
            variaveis_cat = [
                'Q001', 'Q002', 'Q003', 'Q004', 'Q005', 'Q006', 'Q007', 'Q008', 'Q009', 'Q010',
                'Q011', 'Q012', 'Q013', 'Q014', 'Q015', 'Q016', 'Q017', 'Q018', 'Q019', 'Q020',
                'Q021', 'Q022', 'Q023', 'Q024', 'Q025', 'TP_FAIXA_ETARIA', 'TP_SEXO', 'TP_ESTADO_CIVIL',
                'TP_COR_RACA', 'TP_ESCOLA'
            ]
            df_cat = df[variaveis_cat].dropna()

            matriz_v = pd.DataFrame(
                np.zeros((len(variaveis_cat), len(variaveis_cat))),
                columns=variaveis_cat, index=variaveis_cat
            )

            for i, var1 in enumerate(variaveis_cat):
                for j, var2 in enumerate(variaveis_cat):
                    if i >= j:
                        matriz_v.iloc[i, j] = cramers_v(df_cat[var1], df_cat[var2])

            # Salvar matriz V de Cramer
            matriz_v.to_csv('matriz_v_cramer.csv')
            #
            # # Para carregar depois:
            # matriz_v = pd.read_csv('matriz_v_cramer.csv', index_col=0)

            # Extrai pares √∫nicos (sem diagonal e sem duplicatas)
            correlacoes = []
            for i in range(len(variaveis_cat)):
                for j in range(i):
                    correlacoes.append((
                        variaveis_cat[i],
                        variaveis_cat[j],
                        matriz_v.iloc[i, j]
                    ))

            # Ordena do maior para o menor
            correlacoes_ordenadas = sorted(correlacoes, key=lambda x: x[2], reverse=True)

            # Cria DataFrame para exibir
            df_correlacoes = pd.DataFrame(correlacoes_ordenadas, columns=['Vari√°vel 1', 'Vari√°vel 2', 'V de Cramer'])

            st.subheader("Lista de Correla√ß√µes (V de Cramer) - Ordem Decrescente")
            st.dataframe(df_correlacoes, use_container_width=True)


        # Defina as vari√°veis
        notas = ['NU_NOTA_CN', 'NU_NOTA_CH', 'NU_NOTA_LC', 'NU_NOTA_MT', 'NU_NOTA_REDACAO']
        categoricas = [
            'Q001', 'Q002', 'Q003', 'Q004', 'Q005', 'Q006', 'Q007', 'Q008', 'Q009', 'Q010',
            'Q011', 'Q012', 'Q013', 'Q014', 'Q015', 'Q016', 'Q017', 'Q018', 'Q019', 'Q020',
            'Q021', 'Q022', 'Q023', 'Q024', 'Q025', 'TP_FAIXA_ETARIA', 'TP_SEXO', 'TP_ESTADO_CIVIL',
            'TP_COR_RACA', 'TP_ESCOLA'
        ]

        # Remova nulos
        df_mi = df[notas + categoricas].dropna()

        # Codifique as vari√°veis categ√≥ricas como n√∫meros inteiros
        df_encoded = df_mi.copy()
        for col in categoricas:
            df_encoded[col] = df_encoded[col].astype("category").cat.codes

        # Calcule e exiba o ranking para cada nota
        for nota in notas:
            mi_scores = mutual_info_regression(
                df_encoded[categoricas], df_encoded[nota], discrete_features=True, random_state=0
            )
            mi_ranking = pd.Series(mi_scores, index=categoricas).sort_values(ascending=False)
            # Salva o ranking em CSV
            mi_ranking.to_csv(f'mi_ranking_{nota}.csv')

            # mi_ranking = pd.read_csv(f'mi_ranking_{nota}.csv', index_col=0)

            # Exibe no Streamlit
            st.subheader(f"Vari√°veis categ√≥ricas mais correlacionadas com {nota}")
            st.dataframe(mi_ranking.reset_index().rename(columns={'index': 'Vari√°vel', 0: 'Informa√ß√£o M√∫tua'}))





if __name__ == "__main__":
    main()